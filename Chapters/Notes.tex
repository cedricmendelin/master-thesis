\chapter{Papers - Foundation}

\section{Maths Foundation}
\subsection{Hilbers Space}
\subsection{SO(3), S,}
\subsection{LIE Group?}
\subsection{Principle component analysis - PCA}
\subsection{Local PCA}
\subsection{K-means}
\subsection{K-nearest neughboors}


\section{Graph Foundation}
\subsection{Curvature}
\subsection{Geodesic distance}
\subsection{Manifold Assumption}
\subsection{Point Cloud}
\subsection{Laplace}
\subsection{Pooling}
\subsection{Link prediction}
\subsection{Hyper Graphs}
\subsection{Power Iterations}
\subsection{Manifold Learning}
\subsection{Signal Processing}
\subsection{Nonlinear dimensionality reduction}


\section{Diffusion Maps:}
\citet{diffusionMaps}
\cite{diffusionMaps}

Dimensionality reduction:
In essence, the goal is to change the representation of data sets, originally in a form involving a large number of variables, into a
low-dimensional description using only a small number of free parameters.

meaningful structures in data sets:
Analogous to the problem of dimensionality reduction is that of finding meaningful structures in data sets. The idea here is slightly
different and the goal is to extract relevant features out of the data in order to gain insight and understanding of the
phenomenon that generated the data.

Markov Chain:

Random walk:

PageRank:
Stationary distribution of random walk

Kernel eigenmap methods:
- local linear embedding
- Laplacian eigenmaps,
- hessian eigenmaps
- local tangent space alignement

The remarkable idea emerging from these papers is that eigenvectors of Markov matrices can be thought of as coordinates
on the data set. Therefore, the data, originally modeled as a graph, can be represented (embedded) as a cloud of points
in a Euclidean space.
two major advantages over classical dimensionality redution (PCA, MDS):
The first aspect is essential as most of the time, in their original form, the data points do not lie on
 linear manifolds.
 The second point is the expression of the fact that in many
applications, distances of points that are far apart are meaningless, and therefore need not be preserved.

Unnormalized Graph Laplacian:
$L = D - W$

Normalized Graph Laplacian construction:
$L_{sym} = D^{-1/2}LD^{-1/2} = I - D^{-1/2}WD^{-1/2} $
$L_{rw} = D^{-1}L = I - D^{-1}W $

Markov chain has a stationary distribution.
If graph is connected, stationary is unique.
If X is finite, chian is ergodic.

Diffusion distance:
Diffusion map $\psi$ embeds the data into the Euclidean space so that in this space, the Euclidean
distance is equal to the diffusion distance.

Laplace–Beltrami operator on manifolds

What are diffusion maps

\subsection{Vector Diffusion Maps (VDM)}

\cite{vectorDiffusionMaps}
VDMis a mathematical and algorithmic generalization of diffusion maps
and other nonlinear dimensionality reduction methods, such as LLE, ISOMAP,
and Laplacian eigenmaps. While existing methods are either directly or indirectly
related to the heat kernel for functions over the data, VDM is based on
the heat kernel for vector fields.

Main concept:
Edge consists of weight and linear orthogonal transformation.
If linear orthogonal transformation is big, nodes are more like to be equal.
If small, there are different

Diffusion is calculated on vectors fields, where tangets are mapped to the manifold.
A way to globally connect Local PCAs.

\textbf{SNR: signal-to-noise-ration}


LLE:
ISOMAP:
Laplacian eigenmaps:


\subsection{Riemannian Manifold Assumption:}
One of the main objectives in the analysis of a high-dimensional large data set
is to learn its geometric and topological structure. Even though the data itself is
parametrized as a point cloud in a high-dimensional ambient space $R^p$, the correlation
between parameters often suggests the popular “manifold assumption” that
the data points are distributed on (or near) a single low-dimensional Riemannian
manifold Md embedded in Rp, where d is the dimension of the manifold and
$d << p$.


\subsection{Multi-Frequency Vector Diffusion Maps (MFVDM)}
\cite{multiDiffusionMaps}
\textbf{For a direct link between manifold embedding and tomography, very close to what Ivan explained this morning.
If we have a graph denoising method, we will need to compare with this approach 
(or the original vector diffusion maps).}

Basically same as VDM, but with multiple frequencies per edge.

Diffusion maps (DM) only consider scalar weights over the edges and the vector
diffusion maps (VDM) only take into account consistencies
of the transformations along connected edges using only one
representation of $SO(2)$, i.e. $e^{ia_{i,j}}$ . In this paper, we generalize
VDM and use not only one irreducible representation,
i.e. $k = 1$, but also higher order $k$ up to $k_{max}$.


\section{Graph Laplacian Tomography From Unknown Random Projections}
\textbf{A reference that I already mentioned in the first mail:
standard approach that we need to compare with. 
Maybe their setting (2D tomography with unknown angle) is a good setting to start with.
}

\section{denoising}
Recover original image from noisy observation.
Is Achieved by averaging.

- classical local smoothing filters:
    - gaussian filters
    - anisotropic filters
    - Total Variation minimization
    - neighborhood filters

- neighborhood filters (review of image denoising)
- non local means
- functions adapted kernels (nonlinear independent component analysis)


\section{Non-local means}
Image denoising accuratly done.

Better performance, when algorithms tries to correct noise rather than seperate noise
from original image.

Compares similar pixel neighborhoods and assign large weighted for similar pixels.




\section{Learning to Drop}
Graph denoising


\section{WalkPooling}
Image denoising

\section{Point Clouds}
\subsection{Dynamic graph Cnn for learning on point clouds}
One of the few reference related to graph neural network and learning of graph structure. 


\subsection{CryoEm and related}
2. Estimation of Orientation and Camera Parameters from Cryo-Electron Microscopy Images with Variational Autoencoders and Generative Adversarial: 

learning framework where the manifold embedding is estimated.

3. Computational Methods for Single-Particle Cryo-EM: 
review around cryo-EM. 

This reference doesn't talk about manifold embedding, but it is a nice one if you want to know more about 
the acquisition system and standard approaches to solve the cryo-EM problem.


3.bis) Single-Particle Cryo-Electron Microscopy: 
another review similar to the previous one. The section "Mathematical frameworks for cryo-EM data analysis" 
and especially the subsection MRA (multireference alignement) introduce 
a toy model that is related to cryo-EM and where the symmetries are of importance. 

4. Bispectrum Inversion with Application to Multireference Alignment: 
for a paper that introduce several algorithms to solve MRA.