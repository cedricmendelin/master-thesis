\chapter{Assessment criteria}
Written report including: 
\begin{itemize}
    \item Contents of the Master's Thesis project
    \item Project plan
    \item summary of relevant related work
\end{itemize}

\chapter{Foundation}

\textbf{General Questions}
\begin{itemize}
    \item Difference Graph Learning and Graph Representation Learning
    \item What is the overall task?
    \item CryoEm, Datasets, Tasks, etc.?
    \item Benchmarking and Dataset
    \item Plugin for Latex -> Slack
\end{itemize}

\textbf{Trainable Laplacian with folded FSM}
\begin{itemize}
    \item Noisy projection samples? Yes!
    \item What is the underlying problem to solve?
    \item Other graph creation mechanism to consider.
    \item Similarity measure?
    \item Wasserstein Loss function
\end{itemize}

\textbf{Questions GCN:}
\begin{itemize}
    \item During feature propagation, only node features are considered.
    \item Spectral Analysis Chapter in SGCN
\end{itemize}

\textbf{Questions Random projection:}
\begin{itemize}
    \item Assumption about uniform(0, 2$\pi$) and equally spaced $t$?
    \item First-order Chebyshev
\end{itemize}

\textbf{Questions Walk Pooling}
\begin{itemize}
    \item What is vanilla graph classification?
    \item Subgraph classification?
\end{itemize}


\textbf{Next Steps:}
\begin{itemize}
    \item Familiarize with CryoEm
    \item Do some coding
    \item Read paper regarding Manifold Learning
\end{itemize}

\section{Introduction to Graph Learning}

Graph Representational Learning
%\footnote{https://towardsdatascience.com/introduction-to-graph-representation-learning-a51c963d8d11}


\subsection{Spectral graph theory}
Spectral graph theory \cite{SpectralGraphTheory} deals with learning properties and characteristics of graphs, in regard to
the graphs eigenvalues and eigenvectors. 

\subsection{Graph deep Learning}
Deep Learning with graphs.
\textbf{TODO: write more}


\section{Graph Foundations}
A graph is defined as  $G = \langle V,E \rangle$, where $V$ is a set of 
vertices (or nodes) and $E$ is a set of edges (or links). Edges are 
defined as a set of tuples $\langle i,j \rangle$, where $i$ and $j$ determine the 
index of the vertices in the graph.

\subsection{Adjacency Matrix}

The adjacency Matrix of $G$ is then defined as follows:
\begin{equation}
    A_{ij} =    
    \begin{cases}
        %1  & \text{if } \norm{\biggl y_i - y_j \biggr} < \tau\\
        1  & \text{if } \langle i , j \rangle \in E \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

\subsubsection{k-hop neighbourhood}

\subsection{Degree Matrix}

The degree Matrix of $G$ is defined as follows:
\begin{equation}
    D_{ij} =    
    \begin{cases}
        %1  & \text{if } \norm{\biggl y_i - y_j \biggr} < \tau\\
        deg(v_i)  & \text{if } i = j \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

Where $deg(v_i)$ is the degree of the node, formally the number of incoming edges of node $v_i$.

\subsubsection{Adjacency normalization}
We starting calculating with Matrix A, it is sometimes necessary to normalize.
With the degree Matrix $D$ and Adjacency Matrix $A$, we have all information we need.
Mostly, we want to normalize, such that our rows sum to 1.
\begin{equation}
    A_{row-norm} = D^{-1} A
\end{equation}

But we can achieve the same for columns, we just need to swap the two matrices:
\begin{equation}
    A_{col-norm} = A D^{-1}
\end{equation}

And a final, a probably the most useful normalization, is the symmetric normalization:

\begin{equation}
    A_{sym} =  D^{-\frac{1}{2}} A D^{-\frac{1}{2}}
\end{equation}

\textbf{TODO: Add some nice example}

\subsection{Graph Laplacian}
The graph Laplacian is defined as follows:
\begin{equation}
    L = D - A
\end{equation}


\subsubsection{Normalized Graph Laplacian}

Symmetric normalized: $L_{sym} = I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$
Random walk normalized: $L_{rw} = I - D^{-1} A$

\subsubsection{Normalized Graph Laplacian eigen decomposition}

\begin{equation}
    \begin{aligned}
        L_{sym} &= U \Lambda U^T \\
        U &= [u_0, \dots, u_{N-1}] \in R^{N \times N}\\
        \Lambda &= diag \left ( [\lambda_0, \dots, \lambda_{N-1}]  \right ) \in R^{N \times N}
    \end{aligned}
\end{equation}

In this scenario, eigenvectors are also known as \textit{graph Fourier modes}
and eigenvalues are known as the \textit{spectral frequencies}.

Moreover, with the Graph Fourier Transform, we can calculate these values from a symmetric
graph Laplacian.

\subsection{Graph Properties}

\subsubsection{Directed vs. undirected vs. weighted}

\subsubsection{Dense and sparse Graph}
A dense graph is a graph, where the number of edges in close to the maximal number of edges.
Contrarily, a sparse graph only consists of a few edges.

\subsection{Node Properties}
\subsection{Edge Properties}

\subsection{Graph Construction}
\textbf{TODO: KNN}


\subsection{Manifolds}
A manifold is a topological space, where locally Euclidean distances make sense.
More formally, a $n$-dimensional manifold is a topological space where
each point has e neighbourhood, that is homeomorphic to an subset of a n.dimensional
Euclidean space.

Some example for a 1-D manifold is a line or a circle. 2-D manifolds can already become 
pretty complex and are basically any surfaces like planes, sphere but also the torus,
Klein bottle or others.

\subsubsection{Manifold assumption}
The manifold assumption is a popular assumption for high-dimensional datasets.
Even if a given dataset is in high-dimension and consists of many features, one can assume,
that these data points are samples from a low-dimensional manifold, 
which embeds the high-dimensional space.

Therefore, if one can approximate the underlying Manifold, one solved a dimensionality reduction
as we can embed the data points in the low-dimensional manifold space.

There is a complete area of research devotes to this manifold assumption, and basically
we then talk from Manifold Learning. 

\textbf{TODO: write more} \cite{ManifoldLearning}


\subsection{Power Iterations}

Power iteration (also called power method) is a iteratively method, 
which approximates the biggest eigenvalue of a diagonalizable matrix A.

The algorithm starts with a random vector $b_0$ or a approximation of the dominant eigenvector.

\begin{equation}
    b_{k+1} = \frac{Ab_k}{||Ab_k||}
\end{equation}

The algorithm not necessarily converge. The algorithm will converge, if A has an eigenvalue strictly grater than its other eigenvalues
and the initial vector $b_0$ has a component in direction of an eigenvector, associated with the dominant eigenvector.


\section{Noise}
A noisy observation is defined as:
$y_n = y + \eta$

\subsection{Denoising}
When we talk from denoising, we want to reconstruct the true observation 
from a given noisy observation. This reconstruction is done via averaging, which can be performed
locally, by the calculus of variations or in the frequency domain.

\subsubsection{Non local means}
Non local means is a state-of-the-art image denoising method \cite{noneLocalMean}.
In the name of the method are two important concepts, namely the \textit{mean}
and \textit{non local}.

For a given noisy image $v$, the denoised image is defined as:
\begin{equation}
    NL[v](i) = \sum{w(i,j) \; v(j)}
\end{equation}

where $w(i,j)$ is the weight between pixel $i$ and $j$ and fulfils two conditions:
\begin{itemize}
    \item $0 \le w(i,j) \le 1$
    \item $\sum_j{w(i,j) = 1}$
\end{itemize}

The weight can be seen as a similarity measure of the two pixels.
Moreover, these similarities are calculated over square neighbourhoods of the two pixels,
where the l2 norm of the neighbourhood is used.
Similar pixel neighbourhoods have a large weight and different neighbourhoods have a small weight.

More general, the denoised image pixel $i$ is computed as an weighted average of all pixels in the 
image, therefore, in a non local way.



\section{Graph Denoising}
Data acquired by Real-world observations are often noisy, which can lead to poor 
performance on data analysis tasks. This observed data can already be in the form of a graph,
or a graph can be easily constructed. This resulting graph is what we call
a noisy graph, as it includes the noise from the observation.

Graph denoising is the task to reconstruct the original graph from a noisy one.
Therefore, graph denoising can be seen as a pre-processing step, where noisy data is filtered.

Denoising in general has often to do with averaging 
 and graphs are  a well suited data structure for this task\cite{noneLocalMean}.

\subsection{Noisy Graph}
For every noisy graph, there exists an original graph $G = \langle V,E \rangle$.

The noisy graph can be defined as follows:
\begin{equation}
    \begin{aligned}
        G_{noisy} &= \langle V,E_{noisy} \rangle,  \\ 
        \text{ with }  E_{noisy} &= E \setminus  E^{-} \cup  E^{+}, \\ 
         E^{-} & \subseteq E, \\
         E^{+} \cap E &= \emptyset
    \end{aligned}
\end{equation}

Basically, the noisy graph consists of the same vertices as the original graph. From
the original graphs edges, some are removed (denoted by $E^{-}$) and some new edges are added
(denoted by $E^{+}$).

The adjacency Matrix of $G_{noisy}$ is then defined as follows:
\begin{equation}
    \bar{A}_{ij} =    
    \begin{cases}
        %1  & \text{if } \norm{\biggl y_i - y_j \biggr} < \tau\\
        1  & \text{if } \langle i,j \rangle \in E_{noisy} \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

The task of graph denoising, can therefore be written as:
\begin{equation}
    \bar{A} \xrightarrow[method]{Graph-denoising} \tilde{A} \approx A
\end{equation}

Where $\bar{A}$ denotes the noisy input graph, $\tilde{A}$ the denoised
 graph and $A$ the original graph.


\subsection{Graph link prediction}
Link prediction is a task in Graph learning. 
The idea is to predict the existence of a link (edge) between two nodes.
The task can be formulated as a missing value estimation task. A model $M_p$ is learned
from a given set of observed edges. The model finally maps links to probabilities:


\begin{equation}
    M_p : E^{\prime} \rightarrow [0,1]
\end{equation}

Where $E^{\prime}$ is the set of potential links.


We define $U$ as the set of all possible vertices of $G$, therefore $E \subseteq U$.
Obviously, one could see Graph denoising as a link prediction problem.

The difference is, that in link prediction, we learn a model from a set of observed links 
$E_{observed} \subseteq E$ and in Graph denoising we learn the model from 
$E_{observed} \subseteq U$. 

On could also say that link prediction problems are a subset of graph denoising problems.


\section{Deep Learning on Graphs}

\subsection{Graph Convolutional Network}
Graph Convolutional Networks (GCN) \cite{GCN} can be used for many tasks in the field 
of Graph Learning, such as node classification or link prediction. 
Basically, with GCN, a new feature representation is iteratively learned for the node features.

The basic concept is as follows:
For a given graph $G = \langle V,E \rangle$, with node features $X^{N x D}$ and adjacency Matrix $A$
where $N$ denotes the number of nodes and $D$ the number of node input attributes,

a novel node representation $Z^{N x F}$ will be learned, where $F$ is the number of output features.

$Z$ will be learned within a neural network, and every layer can be written by the following, non-linear function:

\begin{equation}
    \begin{aligned}
        H^{l + 1} &= f( H^l, A), \\
        \text{with } H^0 &= X , \\
        H^L &= Z, 
    \end{aligned}
\end{equation}

where $L$ is the number of layers in the neural network.
The model only differ in the choice of $f(\cdot,\cdot)$.

We are ready to define our first GCN. To keep it simple, $f(\cdot,\cdot)$ will be defined as the following:
\begin{equation}
    f( H^l, A) = \sigma (A H^l W^l)
\end{equation} 

Where $\sigma ( \cdot )$ is a non-linear activation function, such as ReLU and $W^l$ is
a weight Matrix of the layer $l$ of the neural network. As \citet{GCN} could show during experiments,
this choice of $f(\cdot,\cdot)$ is already very powerful and leads to state-of-the-art results.

\subsubsection{Renormalization trick}

With this model, we do have two problems and need to refine it further.
First of all, with the multiplication of $A$, we average over the neighbour nodes but
will ignore the node itself. Therefore, self-loops will be added to $A$.
The second problem is, that A is not normalized and if therefore, when multiplying with $A$,
the features of the nodes will change it scale. Therefore, we need to normalize $A$
such that all rows sum to one. This can be done with a simple multiplication with the D.

These two steps are called the Renormalization trick\cite{GCN}.
First of all, we can simple add the self-loops by adding the Identity Matrix to $A$, 
$\hat{A} = A + I$ and $\hat{D}$ is the degree Matrix of $\hat{A}$.
Now, we can achieve a symmetric normalization by multiplying $D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$.

And finally, we can put all things together, and replace $A$ in the original equation:
\begin{equation}
    f( H^l, A) = \sigma (\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} H^l W^l)
\end{equation} 


\subsubsection{Simple Graph Convolutional Network}
Simple Graph Convolutional Network (SGC) \cite{simpleGCN} proposed a simplified version of GCN.
They could verify their hypothesis, that GCN is dominated by the local averaging step and the non-linear 
activation function between layers do not contribute to much to the success of GCN.

This makes the calculation simpler. We denote $S = \hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} $
and can use the fact that in every layer of the neural network, the same computation will take place.

\begin{equation}
    \begin{aligned}
        Z = S \dots S X W^1 W^2 \dots W^L \\
        Z = S^L X W^1 W^2 \dots W^L \\
        Z = S^L X W    
    \end{aligned}
\end{equation}

where $W$ is the matrix of all vector weights.



\subsubsection{Link go Graph Laplacian:}

In the section, we will have a look at the connection between SGC and Graph Laplacian.

We can define $x \in R^n$ as our signals and define the Fourier transform as $\hat{x} = U^T x$
and the inverse as $x = U\hat{x}$. 
With the transform, we can easily switch between spatial and Fourier(spectral) domain.

Further, we can define the graph convolution operation between signal $x$ and filter $g$.

\begin{equation}
    g \star x = U((U^T g) (U^T x)) = U \hat{G} U^T x,
\end{equation}

where $\hat{G}$ is a diagonal matrix where the elements are the 
spectral filter coefficients (eigenvalues?)

The graph convolution can be approximated by the $k$-th order polynomials of Laplacians:

\begin{equation}
    \approx \sum_{i=0}^{k} \Delta^i x = U \left ( \sum_{i=0}^{k}  \Theta_i \Lambda^i \right ) U^T x,
\end{equation}

where $\Delta = D - A$ and $\Theta_i$ are 
filter coefficients which correspond to polynomials of the Laplacian eigenvalues,
 $\hat{G} = \sum_i \Theta_i \Lambda^i$


 In the original \cite{GCN} paper, the approximation is done with $k = 1$ 
 \begin{equation}
     g \star x = \Theta (I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} )x,
 \end{equation}

, where \citet{GCN} further applies the renormalization trick, ending up replacing
$I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ with $\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}}$.

$I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ is also called first-order Chebyshev filter.

%\footnote{https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801}

%not read currently:
%\footnote{https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49}



\subsection{Walk Pooling}

Homophilic and heterophilic are properties of the underlying dataset in link predication. 
A homophilic dataset leads to the tendency to interact with similar nodes. Whereas a heterophilic dataset contrary has the tendency to 
not link, if nodes are similar.

\citet{walkPooling} proposed a new way of link prediction, which works
with a random-walk-based pooling method called WalkPool.

Based on the assumption, that link presence can be predicted 
only based on the information on its neighbours within a small radius $k$,
they calculate subgraphs and extract information from these subgraphs.

\subsubsection{Feature extraction}
Based on the adjacency matrix and some node attributes, 
feature will be extracted with a GNN $f_{\theta}$

\begin{equation}
    Z = f_{\theta} (A,X)
\end{equation}

This step could be achieved by the earlier introduced GCN.

\subsubsection{Subgraph classification}

For candidate link $E^c$, the k-hop enclosing subgraphs
 will be constructed, which allows the transformation
from a link predication problem to a graph classification problem.


From these subgraphs, so called random-walk profiles are calculated:

\subsubsection{Random-walk profiles}

First of all, the node correlation is calculated with two multilayer perceptrons (MLP)

\begin{equation}
    w_{x,y} = \frac{Q_{\theta}(z_x)^T K_{\theta}(z_y)}{\sqrt{F^{\prime\prime}}}
\end{equation}

Then, from all neighbouring nodes ($N(x)$), probabilities are calculated:
\begin{equation}
    p_{x,y} = 
    \begin{cases}
            %1  & \text{if } \norm{\biggl y_i - y_j \biggr} < \tau\\
            [softmax(w_{x,z})_{z \in N(x)}]_y  & \text{if } \langle x , y \rangle \in E \\
            0, & \text{otherwise}
    \end{cases}
\end{equation}

From these calculated probabilities $P$ and its powers, we can derive information for 
graph classification:

Node level features $node^{\tau}$ desribe loop structures arround the candidate link.
Further, link features $link^{\tau}$ give a probability, about a random walk with length
$\tau$ from the two nodes ( ending in a loop). And finally, graph features $graph^{\tau}$ are related to the total
probability of length $\tau$ loops.

\textbf{TODO:}
As we do not know about if the candidate link is present of not, we consider the subgraphs,
consisting of the candidate link and without the candidate link.


We finally can describe WalkPool as:

\begin{equation}
    WP_{\theta}(G, Z) =w_{1,2}(node^{\tau, +}, node^{\tau, -}, link^{\tau, +}, link^{\tau, -}, \delta graph^{\tau})^{\tau_c}_{\tau = 2}
\end{equation}


\section{Graph Laplacian Tomography From Unknown Random Projections}

\citet{LaplaceRandomProjections} introduces a Laplacian-based algorithm, with which 
reconstruction of a planar object from projects at random unknown directions is possible.

Overall, in computerized tomography (CT), reconstruction of an object with only samples of its projections
is a standard problem. In \citet{LaplaceRandomProjections} the problem was extended by the fact,
that the projection angle to the object is unknown.

Formally:
Given $N$ projection vectors $( P_{\theta_i}(t_1), P_{\theta_i}(t_2), \dots, P_{\theta_i}(t_N)$ 
at unknown angles $\{\Theta_i\}^N_{i=1}$ which are drawn from the uniform distribution of $[0, 2\pi]$
and $t_1, t_2, \dots, t_n$ are fixed n points (all equally spaced due to uniform distribution) 
find the underlying density function $\rho (x,y)$ of the object.

\subsection{Radon transform}

The radon transform $P_{\Theta}(t)$ is the line integral of $\rho$
along parallel lines $L$ at angle $\Theta$ and distance $t$ from the orign.

\begin{equation}
    \begin{aligned}
        P_{\Theta}(t) &= \int_L \rho (x,y) ds \\
                      &=  \int_{-\infty}^{\infty} \rho (x,y) \; \delta(x \cos \Theta + y \sin \Theta - t) dx \; dy
    \end{aligned}
\end{equation}

An algorithm for estimating angles from given projections have been introduced by \cite{formerUnkownRandomProjections}.
The introduced algorithm consists of three steps:
\begin{enumerate}
    \item Angle estimation
    \item Angle Ordering
    \item Joint maximum likelihood refinement of angles and shifts
\end{enumerate}

Step 2 was implemented by some nearest neighbour algorithm. In the work of 
\cite{LaplaceRandomProjections}, they introduced a new way of ordering the angles,
using Graph Laplacian.


\subsection{Laplace-Beltrami operator}
\cite{LaplaceRandomProjections} could show, that the graph Laplacian
 approximates the Laplace-Beltrami operator, if data points are uniformly distributed 
 over the manifold.

 Further, they showed that in the case of non-uniformly distributed data points, the Laplacian
 approximated the backward Fokker-Planck operator (which is a generalization of the Laplace-Beltrami operator).
 With that, at least the ordering of the angles can be estimated.

 Finally, with a small normalization of the Laplacian, the Laplace-Beltrami operator can also be 
 approximated in the non-uniform distributed case.

\subsection{Algorithm}

For a given set of projections vector $x_i = ( P_{\Theta_i}(t_1), \dots, P_{\Theta_i}(t_n))$ for $i = 1,2, \dots, mN$

The algorithm proposed in \cite{LaplaceRandomProjections} consists of five steps:

\begin{enumerate}
    \item Double the number of projections to 2mN (due to the fact that projections are symmetric)
    \item Construct the co-called density invariant Graph Laplacian $\tilde{L}$
    \item Compute $\theta_1(i)$ and $\theta_2(i)$ the first two nontrivial eigenvector of $\tilde{L}$
    \item Sort $x_i$ according to $\phi_i = \tan^{-1}(\theta_1(i) \; / \;\theta_2(i))$
    \item Reconstruct image using the sorted projections and estimated angles.
\end{enumerate}

Where $\tilde{L}$ can be constructed by the following way:

\begin{equation}
    \begin{aligned}
        W_{ij} = k \left ( \frac{\left \| x_i - x_j \right \|^2}{2 \epsilon}  \right ), \\
        i, j = 1, \dots, N
    \end{aligned}
\end{equation}

where $||\cdot ||$ is the euclidean-norm, $k$ a semi-positive kernel 
and  $\epsilon > 0$ the bandwidth of the kernel. As mentioned in \cite{LaplaceRandomProjections},
the kernel $k(x) = \exp (-x)$ is a popular choice.

With the newly computed weight Matrix $W$ and the degree Matrix $D$ corresponding to $W$, we can finally 
define $\tilde{L}$.

\begin{equation}
    \begin{aligned}
        \tilde{W} &= D^{-1} W D^{-1} \\
        \tilde{D} &= \text{Degree matrix corresponding to } \tilde{W} \\
        \tilde{L} &= \tilde{D}^{-1} \tilde{W} - I \\
    \end{aligned}
\end{equation}


\section{Folded spectrum Method}
Calculation of eigenvalues and eigenvectors of a given Hamiltonian matrix $H$ 
is a fundamental mathematical problem. Often, we are interested in just the smallest 
values, which can be efficiently computed. But if we are interested in selected values,
this can be hard. $H$ is needed to be diagonalized (bring matrix $H$ into diagonal form) 
which is computationally expensive and for big matrices impossible.

Currently, the best way to solve such problems is the Folded spectrum (FS) \cite{foldedSpectrumMethod} method,
which iteratively solves the problem. During calculation, the eigenvalue spectrum will be folded around a reference 
value $\epsilon$.

\begin{equation}
    v^{t+1} = v^t - \alpha (H - \epsilon I )^2 v^t ,
\end{equation}

with $0 < \alpha < 1$. When $t \rightarrow \infty$, then $v^{\infty}$ should be the 
eigenvector with respect to the reference value $\epsilon$.


%\textbf{A reference that I already mentioned in the first mail:
%standard approach that we need to compare with. 
%Maybe their setting (2D tomography with unknown angle) is a good setting to start with.
%}
