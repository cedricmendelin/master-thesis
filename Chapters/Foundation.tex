\chapter{Foundation}
\label{sec:foundation}

Before dealing with the problem setup of the Master Thesis itself, in the following chapter, a broad foundation
of graphs and Graph Learning will be given. Moreover, some important mathematically concepts and methods will be introduced as well as cryo-EM.
%A more detailed explanation to some sections will follow in the Section~\nameref{sec:relatedWork}

\subparagraph{K-hop neighbourhood:}
\label{sec:K-hop neighbourhood}
The adjacency matrix $A$ has many nice properties, and one is referred to the k-hop neighbourhood. 
When calculating the $k$-th power of $A$, the k-hop neighbourhood of the graph is calculated.
The resulting matrix gives the number of walks of length $k$ from one node to another.

\paragraph{Normalization:}
When starting calculating with matrix A, it is sometimes necessary to normalize.
With the degree matrix $D$ and adjacency matrix $A$, all information for normalization are present.
The normalization can be achieved in a row, column or symmetric way:
\begin{equation}
    \begin{aligned}
        A_{row-norm} &= D^{-1} A \\
        A_{col-norm} &= A D^{-1} \\
        A_{sym}      &=  D^{-\frac{1}{2}} A D^{-\frac{1}{2}}    
    \end{aligned}
\end{equation}

The normalization of the row and column will normalize the row and column to sum to $1$ respectively.
The symmetric normalization is well suited for undirected graphs, as it preserve the nice symmetric structure matrices.




\paragraph{Normalized Graph Laplacian:}
During computation, it is often needed to have a normalized version of the Graph Laplacian: \newline
\begin{equation}
    \label{eq:normalizedGraphLaplacian}
    \begin{aligned}
        L_{sym} &= I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}
        L_{rw}  &= I - D^{-1} A,
    \end{aligned}
\end{equation}
where $L_{sym}$ is a symmetric normalization and $L_{rw}$ is called a random walk normalization.


\section{Math Foundation}
In the following section, some mathematically concepts and methods will be explained.

\subsection{Embedding}
Mathematically, an embedding $f: X \rightarrow Y$ is defined as a structure-preserving mapping from one domain to another.

In graph theory, a graph embedding in the mapping from the graph $G$ to a surface structure $\Sigma$. 


\subsection{Manifolds}
A manifold is a topological space, where locally Euclidean distances make sense.
More formally, a $n$-dimensional manifold is a topological space where
each point has e neighbourhood, that is homeomorphic (mapping which preserves topological properties) to an subset of a n-dimensional
Euclidean space.

Some example for a 1-D manifold is a line or a circle. 2-D manifolds can already become 
pretty complex and are basically any surfaces like planes, sphere but also the torus,
Klein bottle or others.

\paragraph{Manifold assumption:}
\label{sec:manifoldAssumption}

The manifold assumption is a popular assumption for high-dimensional datasets.
Even if a given dataset is in high-dimension and consists of many features, one can assume,
that these data points are samples drawn from a low-dimensional manifold, 
which embeds the high-dimensional space.

Therefore, if one can approximate the underlying Manifold, one solved the dimensionality reduction
as one can embed the data points in the low-dimensional manifold space.

There is a complete area of research devoted to this manifold assumption called Manifold Learning\cite{ManifoldLearning}.




\section{Graph Learning}
As already mentioned, Graph Learning is a popular research area and got a lot of attention in recent years.
It is a new way of applying Machine Learning (ML) with graphs as a data structure and many algorithms emerged from ML.

A lot of real data can be modelled as graphs. The data could have graph structure, like social networks. 
Or a graph can be artificially constructed with methods like k-nearest neighbours (KNN) or with some other similarity
measure.

%\subsection{Graph Learning Tasks}
\paragraph{Graph Learning Tasks:}
When a graph is available, one can start using Graph Learning algorithms for solving tasks.
Popular tasks are node classification or link prediction within a graph. One tries to learn from node and edge features 
as well as the topology of the graph and tries to map information to a model, which allows prediction or classification.

Another popular task in Graph Learning is community detection, where the aim is to identify cluster of nodes within the input graph.

Further, graphs are highly popular for dimensionality-reduction. In higher dimensions, the euclidean distance is not helpful and therefore,
algorithms for reducing the dimensionality, such that euclidean distance make sense again. are needed. 
Graph algorithms provide a helpful tool in such scenarios, as ordinary algorithms like principle component analysis (PCA) fail.

\paragraph{Algorithm categories}

There are many algorithmic approaches, how to exploit graphs. 

\textit{Graph Deep Learning} is a derivation from Deep Learning. 
Basically, in Graph Deep Learning, Deep Learning algorithms are extended for the usage with graphs.
After all, a model or some feature will be learned within a neural network, suitable for working with graphs.

\textit{Spectral graph theory}\cite{SpectralGraphTheory} deals with learning properties and characteristics of graphs, in regard to
the graphs eigenvalues and eigenvectors. 

\textit{Manifold Learning }\cite{ManifoldLearning} is a popular approach for dimensionality reduction on graphs. 
Using the manifold assumption section~\ref{sec:manifoldAssumption}, an embedding of the graph for lower dimension is calculated,
which can preserve most of the information, of the original graph.

Further, \textit{Random Walks} is a concept, which is often used in Graph Learning. 
It is used to exploit topological information of a graph by randomly "walking" (use edges to move from on node to another)
over the graph. With sampling a lot of these walks, one can infer information about the graphs topology.


