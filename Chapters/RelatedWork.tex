\chapter{Related Work}
\label{sec:relatedWork}

In the following section, related work will be introduced.

\section{Graph Deep Learning}
As we have seen in the Foundation chapter~\ref{sec:foundation}, one can define the problem of Graph Denoising as
a way of link predication. The state-of-the-art method for solving link prediction are graph deep learning approaches.
Graph deep learning is a fast evolving field in research. With Graph Neural Networks (GNN)\cite{GNN} the framework
for GNN has been established. 

Using Graph Convolutional Networks (GCN) \cite{GCN} is a popular way for graph feature extraction. 
Basically, with GCN a new feature representation is iteratively learned for the node features (edges features are not taken into account).
It can be seen as an averaging of nodes over their neighbourhood where all the neighbours get the same weight combined with some non-linear activation. 
To consider the node itself in the averaging process they apply to so-called "Renormalization trick", where self-loops are added to the 
adjacency matrix and after every layer, a normalization step is applied.The topology of the graph will not be adjusted during the learning process.

\citet{GAT} extended the concept of GCN with attention and not all the neighbouring nodes get the same weight (attention).
Simple Graph Convolutional Network (SGC) \cite{simpleGCN} proposed a simplified version of GCN.
They could verify their hypothesis that GCN is dominated by the local averaging step and the non-linear 
activation function between layers do not contribute to much to the success of GCN. 
Therefore, it can be seen as a way of power iteration over the adjacency matrix with normalization in every layer.
\citet{dynamicGCN} proposed a extended version of GCN by not operating on the same graph in every layer but adopting
the underlying graph topology layer by layer.


%\section{Manifold Learning}
%Manifold learning is a popular method for dimensionality reduction for high-dimensional datasets.
%\cite{Isomap}
%\cite{LLE}
%\cite{LaplacianEigenmaps}

\section{Denoising}

Denoising is an important part in practical application of ML, as observed signals are often noisy.
Especially in computer vision it has a high importance, where observation noise in images is a major issue.


Non local means is a state-of-the-art image denoising method \cite{noneLocalMean}.
In the name of the method are two important concepts, namely the \textit{mean}
and \textit{non local}.

For a given noisy image $v$, the denoised image is defined as $NL[v](i) = \sum{w(i,j) \; v(j)}$.
where $w(i,j)$ is the weight between pixel $i$ and $j$. The weight can be seen as a similarity measure of the two pixels.
Moreover, these similarities are calculated over square neighbourhoods of the two pixels,
where the l2-norm of the neighbourhood is used.
Similar pixel neighbourhoods have a large weight and different neighbourhoods have a small weight.
More general, the denoised image pixel $i$ is computed as an weighted average of all pixels in the 
image, therefore, in a non local way.

\citet{learningToDrop} introduced PTDNET, a way of topological denoising in graphs.
It can be seen as two neural networks, where the first is called denoising network and the second is a GNN.
Firstly, in the denoising network noisy edges will be removed due to
sampling subgraphs from a learned distribution on edges. The aim is to remove 
irrelevant edges. Further, in the GNN the node representation of the denoise graph is learned.



\section{Problem setup section}
In the last section of related work, papers which aim to solve part of the Master Thesis problem will be introduced.

\citet{LaplaceRandomProjections} introduces a Laplacian-based algorithm, with which 
reconstruction of a planar object from projections at random unknown directions is possible.
It can be seen as an algorithm for solving classical tomography, where the problem is extended
by the fact that projection angles are unknown. 
They could order projections of the Shepp-Logan phantom by using the Graph Laplacian 
and used this fact to successfully reconstruct the phantom, even if observations are noisy. 
The proposed algorithm is not directly applicable to the reconstruction of cryo-EM 
as projection in 3D do not have a proper ordering.

\citet{cryoEmVAE-GAN} introduced a way of estimation camera parameter (PSF) as well
as the unknown rotation in cryo-EM reconstruction problem. 
They combined a Variational Autoencoder (VAE) with a Generative Adversarial Network (GAN).
The VAE can be seen as learning a manifold to fit the observations
which was used as an input to the GAN.


Diffusion maps\cite{diffusionMaps} (DM) is a non-linear approach for calculating low-dimensional manifolds
for (high-dimensional) datasets. 
The process is based on the concept of random-walks and works as follows:
First of all, matrix $P$ is calculated which contains the probability from moving from one node to another.
Similar to the k-neighbourhood of $A$ introduced in the foundation chapter, with power of $P^t$
probabilities of reaching node $i$ in $t$ hops can be calculated. 
The diffusion distance at time $t$ can be seen as a distance measure for two nodes in the diffusion space
with added connectivity. It approximates the euclidean distance in the diffusion space and therefore 
allows to compare node embeddings regarding their euclidean distance.
With diagonalization of $P$ and selecting the first $n$ largest eigenvalues/eigenvectors the embedding can be computed.
Vector DM (VDM)\cite{vectorDiffusionMaps} generalize the concept of DM for vector fields.
Multi-Frequency Vector Diffusion Maps (MFVDM)\citet{multiDiffusionMaps} 
can be seen as an extension to Vector Diffusion Maps (VDM)\cite{diffusion}, 
which works well even on highly noisy environments.
\cite{multiDiffusionMaps} was successfully used in cryo-EM setting, where it was used for denoising purpose\cite{cryoEmMutliDM}.


