\chapter{Introduction}
\label{sec:introduction}

Inverse Problems aim to estimate an original signal that went through a system, 
based on potentially noisy output signal observations.
They are widely used throughout different science directions, such as Machine Learning (ML),
Signal Processing, Computer Vision, Natural Language Processing and others.
ML is one tool to model and solve such inverse problems.


\bigskip

In recent years, graphs got a lot of attention in ML and Graph Machine Learning is one of the most promising research areas.
Graphs are a well suited data structure, simple but with high expressiveness. 
Especially data for which single data points tend to have a relation to other data points, graphs with its nodes and vertices are the perfect tool
to capture these relationships. 
Data is in a graph structure already, like social networks, or they can be artificially constructed for arbitrary datasets.
Besides, for some scenarios, ordinary ML algorithms fail, but Graph ML approaches have great success, e.g. dimensionality reduction for high-dimensional data.


\bigskip

Cryo-Electron Microscopy (cryo-EM) is a molecular imaging method and gained a lot of attention in recent years. 
Molecules are frozen and imaged through an electron microscope.
Due to ground-breaking improvements regarding hardware and data processing, the field of research
has highly improved. In 2017, pioneers in the field of cryo-EM got the 
Nobel Prize in Chemistry\footnote{https://www.nobelprize.org/prizes/chemistry/2017/press-release/}.
Today, using cryo-EM, molecular structures can be observed with near-atomic resolution.
The big challenge with cryo-EM is enormous noise and unknown observation angles.

Computed tomography (CT) is a similar to cryo-EM, but reconstruction is slightly easier
as the problem is in 2D and observation angles are known.
The overall goal of this Thesis is to introduce an algorithm that works with CT, but 
can conceptually be extended to work in 3D, thus for cryo-EM. 
Additionally, the focus is on the high noise domain, as current available reconstruction algorithms
start to fail when dealing with too much noise, to be more precise SNR between $[-10, 0]$.

\clearpage

As a result of this Thesis, a Graph Neural Network architecture is proposed, which is called \textit{GAT-Denoiser}.
GAT-Denoiser aims to denoise noisy observations from CT to improve overall reconstruction quality.
For CT, observations angles are assumed to be known.
In the GNN architecture, convolution and Graph Attention Network (GAT) is used to denoise observations.
In addition, an end-to-end learning approach with U-Net is used to further improve reconstruction quality.

GAT-Denoiser was evaluated on the LoDoPaB-CT dataset~\cite{lodopab-dataset}.
I could show that all three components contribute to learning the best GAT-Denoiser model.
Moreover, GAT-Denoiser outperformed BM3D~\cite{bm3d} as well as U-Net~\cite{unet-tomography},
where observation signal-to-noise-ration (SNR) is between 0 dB and -15 dB.
Compared to the best performing baseline BM3D, GAT-Denoiser could improve reconstruction SNR 
by 379.9\% for observation SNR -15 dB.
The best GAT-Denoiser models could be established, when first, GAT-Denoiser is trained with a fixed U-Net model.
After some learning, in a second step, GAT-Denoiser and U-Net have been trained jointly.

\bigskip

The report is structured as follows: 

First, notation applied throughout this Thesis is introduced in Chapter~\ref{sec:notation}.
Then, Chapter~\ref{sec:imaging} presents the two molecular imaging methods
CT and cryo-EM and a mathematical abstraction for observation and reconstruction is introduced.
Further, Chapter~\ref{sec:manifold_and_graphs} is dedicated to manifolds and graphs,
and how a meaningful embedding for CT and cryo-EM can establish. 
The main contribution is presented in Chapter~\ref{sec:contribution}, 
where GAT-Denoiser is introduced and results are presented in Chapter~\ref{sec:results}.
Finally, conclusion and future work are presented in Chapter~\ref{sec:Conclusion}.

\chapter{Notation}
\label{sec:notation}

In this chapter, some basic terms are explained and their notation in this report is defined.

\section{Molecular Imaging Methods}
In molecular imaging methods CT and cryo-EM, observations are collected from a biological sample. 
These observations are noisy and from these noisy observations, reconstruction to approximate the biological sample is desired.
Throughout this Thesis, notation $p$ for noiseless observation and $y$ for observation with noise is used.
In practice, $p$ is not observable directly and observed signal $y$ needs to be denoised.
Further, $x$ is used for the biological sample and $x^{\prime}$ defines approximation by the reconstruction algorithms.
In addition, $N$ is used for the number of observations and $M$ as the observation dimension.
Consequently, $y_i \in \mathbb{R}^M$ determines $i$-th observation and $y_i[j] \in \mathbb{R}$ the $j$-th element of $y_i$,
with $ 1 \leq i \leq N \text{ and } 1 \leq j \leq M$. The same is true for $p$ with $p_i$ and $p_i[j]$. 


\paragraph{Signal-to-noise-ratio}
Signal-to-noise-ratio (SNR) is a measure used in Signal Processing. 
It compares the power of an input signal to the power of noise and is typically given in decibel (dB), with
$\text{SNR}_{dB} = 10 \log_{10} \left(  \frac{P_{signal}}{P_{noise}} \right)$.
An SNR of 0 dB and lower indicates more noise than signal.

From a given clean image and its noisy version, SNR can be determined.
During this Thesis, the term SNR is used as quality measure for reconstruction and to indicate, how much noise is present in $y$.
To make notation clear, \textit{SNR} refers to SNR computed from $x^{\prime}$ and $x$.
Contrary, $\textit{SNR}_y$ is used to express the level of noise in $y$, 
where SNR is computed from $y$ and $p$.

\section{Graph}
A graph is defined as $G = \langle V,E \rangle$, where $V$ is a set of 
vertices (or nodes) and $E$ is a set of edges (or links). 
Edges are defined as a set of tuples $(i, j)$, where $i$ and $j$ determine 
the index of vertices in the graph.

\paragraph{Graph properties:}
A graph can be either \textit{directed} or \textit{undirected}. 
In a directed graph, an edge connects explicitly from one node to another, consequently edge $(i, j) \neq (j, i)$. 
In an undirected graph, edges have no direction and ordering does not matter, therefore $(i, j) = (j, i)$.
Throughout this Thesis, undirected graphs are considered and when writing from a graph, is refers to an undirected graph.

The \textit{neighborhood}, denoted by $\mathcal{N}(i)$, of a node $i$  is defined as all adjacent nodes.
In other words, there is an edge between neighborhood nodes and $i$. 
Further, edges can have \textit{weights}, which is a method to define the importance of neighbors, resulting in a \textit{weighted} graph.
\textit{Degree} of a node are the number of incoming edges.

\paragraph{Adjacency Matrix:}
The (binary) adjacency matrix of graph $G = \langle V, E \rangle$ is defined as follows:
\begin{equation}
    \label{eg:AdjacencyMatrix}
    A_{ij} =    
    \begin{cases}
        1  & \text{if } (i, j) \in E \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

Matrix $A$ has dimension $\mathbb{R}^{N \times N}$ with $N$ as number of nodes
and indices of $A$ correspond to nodes in $V$.
If there exists an edge between two nodes, entry in $A$ will be set to $1$, otherwise to $0$.
This leads to an unweighted graph, as weights of all edges will be $1$.
When $G$ is undirected, corresponding adjacency matrix will be symmetric. 
Eigenvalues of $A$ are called \textit{spectrum} of $G$.

\paragraph{Graph construction with k-NN:}
K-nearest-neighbors (k-NN) is a graph construction algorithm. The distance between
nodes is calculated with a distance measure (e.g. Euclidean distance) and 
for every node, $\mathcal{N}_i$ is defined as $k$ nodes with the smallest similarity measure.
During this Thesis, when writing from $k$ it refers to k-NN parameter $k$.


\section{Maths}
\paragraph{Convolution:}
During this Thesis, symbol $\star$ is used for the convolution operator.

\paragraph{Concatenation:}
$\parallel$ is used for the concatenation operator, which combines vectors end-to-end. 
For vectors $x=(x_0, \dots, x_n)$ and $y=(y_0, \dots, y_n)$ concatenation is defined as 
$ x \bigparallel y = z$ with $z=(x_0, \dots, x_n, y_0, \dots, y_n) $. 
The operator can be applied to an arbitrary number of vectors.
Consider $N$ vectors $v_1, \dots, v_n$, concatenation is written as  $\bigparallel^N_{i=1} v_i$.
