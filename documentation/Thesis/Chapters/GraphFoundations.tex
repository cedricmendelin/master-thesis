\chapter{Graph Denoising}
\label{sec:graphFoundations}
    

The following chapter establishes connection between graphs and denoising in high-noise 
domains, such as cryo-EM.
First, a broad definition of graphs is given and further, the term "Graph Denoising" is
introduced and explained. Finally, connection to Graph Laplacian and Manifolds is established
and different opportunities exploiting for a good denoising algorithms are shown.

\section{Graph Foundations}
Real world data can be in graph structure, like social networks, citation networks,
protein interaction networks or google search. 
If data is not available in graph structure, a graph can be artificially constructed with methods like k-nearest neighbours (k-NN) or others.
A general framework for graph construction is introduced in section~\ref{sec:graphConstruction}.

\paragraph{Graph Learning:} Graph Learning is a hot research area and got a lot of attention in recent years.
It is a way of applying ML on graphs and algorithms emerged from ML but also other fields.
When a graph is available, one can start using Graph Learning algorithms for solving tasks.
Popular tasks are \textit{node classification} or \textit{link prediction}, where model is learned from node and edge features 
as well as topology. The model can than be used for prediction or classification.
Another common task is \textit{community detection}, where the aim is to identify cluster of nodes within the input graph.
Further, graphs are highly favoured for \textit{dimensionality-reduction}, where 
graph algorithms provide a helpful tool, as ordinary algorithms like principle component analysis fail to 
establish a meaningful dimensionality-reduction.

\subsection{Graph definition}
A graph is defined as $G = \langle V,E \rangle$, where $V$ is a set of 
vertices (or nodes) and $E$ is a set of edges (or links). 
Edges are defined as a set of tuples $(i, j)$, where $i$ and $j$ determine 
the index of vertices in the graph.

\paragraph{Graph properties:}
A graph can be either \textit{directed} or \textit{undirected}. 
In a directed one, an edge connects explicitly from one node to another, which means that edge $(i, j) \neq (j, i)$. 
In undirected graphs, the edge was no direction and ordering does not matter, therefore $(i, j) = (j, i)$.

The \textit{neighbourhood}, denoted by $\mathcal{N}(i)$, of a node $i$  is defined as all adjacent nodes.
In other words, there is an edge between neighbourhood nodes and $i$. 
Further, edges can have \textit{weights}, which is a method to define importance to neighbours of a node.
If edges are dealing with weights, the term \textit{weighted} graph is used.
The \textit{degree} of a node are the number of incoming edges.

\paragraph{Adjacency matrix:}
To do calculations with graphs, it is common to translate graphs in a matrix, 
such as the adjacency matrix.
The (binary) adjacency matrix of graph $G$ is defined as follows:
\begin{equation}
    \label{eg:AdjacencyMatrix}
    A_{ij} =    
    \begin{cases}
        %1  & \text{if } \norm{\biggl y_i - y_j \biggr} < \tau\\
        1  & \text{if } (i, j) \in E \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

Matrix $A$ has dimension $\mathbb{R}^{N \times N}$ with $N$ as number of nodes
and indices of $A$ correspond to nodes $V$.
If there exists an edge between two nodes, entry in $A$ will be set to $1$, otherwise to $0$.
This leads to an unweighted graph, as weights of all edges will be $1$, 
but could easily be extended by assigned not just values of $1$ or $0$. 
When the graph is undirected, the corresponding adjacency matrix will be symmetric. 
Eigenvalues of $A$ are called \textit{spectrum} of the graph.


\subsection{Graph construction}
\label{sec:graphConstruction}
When data is not available as a graph, it can be easily constructed.
Consider data from space $\Omega \subset \mathbb{R}^M $, but could basically be any arbitrary space.
Then, each node is associated with some element $p \in \Omega$. 
Further, graph $G$ can be constructed by using:

\begin{equation}
    \label{eq:graphConstruction}
    A_{ij} =    
    \begin{cases}
        1  & \text{if } dist(p_i, p_j) < \tau\\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

where $p_i$, $p_j$ are nodes from indices $ij$ and $j$, $dist$ corresponds to a similarity (or distance) measure and $\tau$ is a threshold, 
when to consider two nodes to be adjacent.
K-NN is one possible implementation of a graph construction algorithm, 
where for every node, $k$ neighbours will be defined.
The neighbourhood $\mathcal{N}_i$ of node $i$ is defined as $k$ nodes with smallest similarity measure.

\paragraph{Noise regime}
In the case of noise, $p$ is not available directly.
Measurements will give access to $y = p + \eta$ where $y,p \in \Omega$ and the noise $\eta$ is assumed to be drawn from gaussian distribution $\mathcal{N} \sim (0,\sigma^2)$.
The \textit{noisy graph} $G_0$ can be constructed as in equation~\ref{eq:graphConstruction}, but replacing $y$ with $p$:
\begin{equation}
    \label{eq:graphConstructionNoise}
    A_{0_{ij}} =    
    \begin{cases}
        1  & \text{if } dist(y_i, y_j) < \tau\\
        0, & \text{otherwise}
    \end{cases}
\end{equation}


\section{Graph Denoising definition}

First of all, \textit{Graph Denoising} is not a common term in literature.
In previous section, noisy graph $G_0$ was introduced and goal is to denoise this graph,
which means to estimate original graph $G$ from a given noisy graph $G_0$. 
This is our definition for Graph Denoising, which is rather related to signal or image denoising.
Reconstruction of a true signal given noisy observation signal is done via averaging, that can be performed
locally, by the calculus of variations or in the frequency domain\cite{noneLocalMean}. 

\paragraph{Noisy Graph:}
For every noisy graph there exists an original graph $G = \langle V,E \rangle$.
The noisy graph $G_0$ can further be defined as $G_0 = \langle V, E_0 \rangle$,  
 where $E_0 = E \setminus  E^{-}_0 \cup  E^{+}_0$ with $E^{-}_0 \subseteq E$ and $E^{+}_0 \cap E = \emptyset$.

$G_0$ consists of same nodes $V$ as original graph $G$. 
From $E$ some edges are removed (denoted by $E^{-}_0$) and some are added
(denoted by $E^{+}_0$), which results is edges $E_0$.

Graph Denoising can therefore be written as $GD: A_0 \mapsto \tilde{A} \approx A$,
where $A_0$, $\tilde{A}$, $A$ denotes adjacency matrix from noisy input graph, denoised graph and original graph respectively.


\paragraph{Connection to link prediction:}
Link prediction is a task in Graph Learning. 
The goal is to predict existence of a link between two nodes.
The task can be formulated as a missing value estimation task. A model $M_p$ is learned
from a given set of observed edges. The model finally maps links to probabilities
$M_p : E^{\prime} \rightarrow [0,1]$ where $E^{\prime}$ is the set of potential links.

Further, $U$ determines the set of all possible vertices of $G$, therefore $E \subseteq U$.
Clearly, Graph Denoising can be seen as a link prediction problem.
The difference is, that in link prediction a model from a set of observed links is learned
$E^{\prime} \subseteq E$ and in Graph Denoising model is learned from 
$E^{\prime} \subseteq U$. 

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    On could also say that link prediction problems are a subset of graph denoising problems.
\end{tcolorbox}

\subsection{Non-local means:}
In the following section, a short introduction to the 
state-of-the-art image denoising method non-local means is given \cite{noneLocalMean}.
For a given noisy image $v$, the denoised image is defined as $NL[v](i) = \sum{w(i,j) \; v(j)}$,
where $w(i,j)$ is weight between pixel $i$ and $j$. 
Weight can be seen as similarity measure of pixels, which are calculated over square neighbourhoods.
Similar pixel neighbourhoods have a large weight and different neighbourhoods have a small weight.
More general, denoised image pixel $i$ is computed as an weighted average of all pixels in the 
image, therefore, in a non-local way.

Non-local means is not a denoising algorithm, which works with graph as a data structure.
But, it uses a neighbourhood for averaging, which shows great potential of graphs
as a data structure for denoising, as graphs can represent neighbours and neighbourhoods really well.


\section{Graph Deep Learning}
\label{sec:graph_depp_learning}
As already mentioned, Graph Denoising can be seen as a way of link predication. 
The state-of-the-art method for solving link prediction are \textit{Graph Deep Learning} approaches.
Graph Deep Learning is a fast evolving field in research. With Graph Neural Networks (GNN) \cite{GNN} the framework
for neural networks with graphs has been established. 

Using Graph Convolutional Networks (GCN) \cite{GCN} for graph feature extraction is a popular way. 
With GCN a new feature representation is iteratively learned for the node features (edge features are not considered).
It can be seen as an averaging of nodes over their neighbourhood where all neighbours get the same weight combined with some non-linear activation (e.g. ReLU). 
To consider the node itself in averaging they apply the so-called "Renormalization trick", where self-loops are added to the 
adjacency matrix and after every layer, a normalization step is applied. 
The topology of the graph will not be adjusted during  learning process.

\citet{GAT} extended the concept of GCN with attention and not all the neighbouring nodes get the same weight (attention).
Simple Graph Convolutional Network \cite{simpleGCN} proposed a simplified version of GCN.
They could verify their hypothesis that GCN is dominated by local averaging step and non-linear 
activation function between layers do not contribute to much to the success of GCN. 
Therefore, it can be seen as a way of power iteration (see \ref{sec:powerIterations} for further information) over the adjacency matrix with normalization in every layer.
\citet{dynamicGCN} proposed an extension to GCN by not operating on the same graph in every layer but adopting
underlying graph topology layer by layer.

\section{Graph Laplacian \& Manifolds}
\subsection{Graph Laplacian}
Graph Laplacian (GL) is a matrix that represents a graph and can be used to find many important properties.
It is a very powerful tool and therefore, a complete section is dedicated to it.
A good introduction and overview can be found by \cite{tutorialSpectralClustering, SpectralGraphTheory}. 

The matrix is defined as follows:
\begin{equation}
    L = D - A,
\end{equation}

where $A$ is the adjacency matrix and $D$ the degree matrix (diagonal matrix with degree of nodes as entries).

\subsection{Manifolds}
\label{sec:manifolds}

In high-dimensional data Euclidean distances are not meaningful,
in the sense that they will not capture similar data points well.
Graph Laplacian can be used to compute a \textit{Manifold}, which can help in such scenarios. 
In manifold space, Euclidean distances make sense again. 
Let manifold $M$ be defined as $\mathcal{M} = \{ f(x), f \in C^K, f: \mathbb{R}^D \to \mathbb{R}^d \}$.
Manifolds are a well established mathematical concept. In the Master Thesis, only 
$C^k$ differentiable d-dimensional manifolds defined by $\mathcal{M}$ are considered. 
When $d \ll D$, manifolds define a \textit{low-dimensional embedding}, which maps from high-dimensional space 
$\mathbb{R}^D$ to low-dimensional space $\mathbb{R}^d$.

Lets give two popular examples of manifolds, namely the \textit{circle} and the \textit{sphere}.
The circle is a 1D manifold, where $d=1$ and $D=2$. A sphere is a 2D manifold, with $d=2$ and $D=3$.
In figure~\ref{fig:circle_sampling}, 200 samples are drawn from a uniform distribution of the unit-circle manifold
and in figure~\ref{fig:sphere_sampling}, 800 samples are drawn from a uniform distribution of the unit-sphere manifold,
as well as the sphere itself.

\begin{figure}[H]
    \centering
    \subbottom[Circle samples
        \label{fig:circle_sampling}]
        {\includegraphics[width=0.4\textwidth]{circle_sampling.png}}
    \subbottom[Sphere samples      
    \label{fig:sphere_sampling}]
        {\includegraphics[width=0.4\textwidth]{sphere_sampling.png}}
    \caption{Samples drawn from 1D and 2D manifold.}
\end{figure}

% One popular algorithm for calculating manifolds is diffusion maps \cite{diffusionMaps}, 
% which is a non-linear approach for calculating low-dimensional manifolds
% for (high-dimensional) datasets, using Graph Laplacian.
% Vector diffusion maps \cite{vectorDiffusionMaps} generalize the concept of diffusion maps for vector fields.
% Multi-frequency vector diffusion maps \cite{multiDiffusionMaps} 
% can be seen as an extension to vector diffusion maps, which works well even on highly noisy environments.
% \citet{cryoEmMutliDM} successfully applied multi-frequency vector diffusion Maps in cryo-EM setting,
%  where it was used for denoising purpose.

\subsection{Manifold assumption}
\label{sec:manifoldAssumption}
Manifold assumption is a popular assumption for high-dimensional datasets.
For a given dataset in high-dimension, one can assume that data points are samples drawn from a low-dimensional manifold,
that embeds the high-dimensional space. 
Therefore, if underlying manifold can be approximated, a dimensionality reduction
is established as one can embed data points in the low-dimensional manifold space.
There is a complete area of research devoted to this manifold assumption called Manifold Learning\cite{ManifoldLearning}.

\subsection{GL-manifold calculation}
\label{sec:manifold_calculation}
A simple low-dimensional embedding (manifold) of a dataset can be calculated with the Graph-Laplacian 
by the following:

\begin{enumerate}
    \item Construct k-NN graph from observations (see section~\ref{sec:graphConstruction}).
    \item Calculate the (normalized) Graph Laplacian.
    \item Extract the second, third (and fourth) smallest eigenvectors.
\end{enumerate}



\subsection{Manifold of computed tomography and cryo-EM}

Therefore, it can be observed how the GL-manifold of classical tomography and cryo-EM objects look like.
In the following, the Shepp-Logan phantom is again used as an example of classical tomography
and the low-dimensional embedding is calculated from the sinogram, following instructions from 
section~\ref{sec:manifold_calculation}.

Radon Transform, parameters $\theta$ and $s$ where speciefied as $\theta \in \mathbb{R}^{500}$ as evenly spaced
between $[0, 2 \pi]$ and $s$ was set to $200$. 

In figure~\ref{fig:clean_manifold}, GL-manifold calculated from clean sinogram and k=2 can be seen.
The GL-manifold looks like a perfect circle. Further, noise was added to the sinogram 
to reach SNR 20dB and again, GL-manifold was computed with $k=2$, which failed (figure~\ref{fig:noisy_manifold_k2}).
But, if neighbourhood is not too strictly defined and k is increased to 4, the GL-manifold looks more circle like 
(figure~\ref{fig:noisy_manifold_k4})

\begin{figure}[H]
    \centering
    \subbottom[Clean sinogram manifold, k=2 \label{fig:clean_manifold}]
        {\includegraphics[width=0.18\textwidth]{phaton_clean_manifold.png}}
    \subbottom[Noisy sinogram manifold, k=2 \label{fig:noisy_manifold_k2}]
        {\includegraphics[width=0.18\textwidth]{phaton_noisy_manifold_k2.png}}
    \subbottom[Noisy sinogram manifold, k=4 \label{fig:noisy_manifold_k4}]
            {\includegraphics[width=0.18\textwidth]{phaton_noisy_manifold_k4.png}}
    \caption{Shepp-Logan phantom sinogram manifolds.}
\end{figure}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    In the field of classical tomography and cryo-EM, the underlying low-dimensional GL-manifold is well defined for none-noisy data.
    In the 2D case of classical tomography, the underlying GL-manifold is a circle, whereas in 3D case of cryo-EM the GL-manifold
    is defined as a sphere.
    This fact can be exploited during learning.
\end{tcolorbox}

\paragraph{Manifold quality:}

Finding a good GL-manifold is not easy and in our case, the GL-manifold is dependent on parameter $k$ during graph construction
as well as parameter $\theta$, $s$ and noise $\eta$ for obtaining the sinogram.

K is an important parameter for building up a graph from data. If set too low, neighbours
do not capture similar data well as too few connections are drawn. 
Further, if k is set too high, the strength of a neighbour 
is weakend and data is not well explained as well.
In figure~\ref{fig:clean_manifolds}, GL-manifold computed by clean sinogram and k from 2 to 10 is illustrated.
One can see, that from $k <= 4$ GL-manifold results in a perfect circle and with $k >  4$ is moves 
further away from the circle. 


\begin{figure}[H]
    \centering
    \subbottom[Clean sinogram GL-manifolds \label{fig:clean_manifolds}]
        {\includegraphics[width=0.45\textwidth]{phaton_clean_manifold_kdifferent.png}}
    \subbottom[Noisy sinogram GL-manifolds \label{fig:noisy_manifolds}]
        {\includegraphics[width=0.45\textwidth]{phaton_noisy_manifold_kdifferent.png}}
    \caption{Shepp-Logan phantom sinogram manifolds.}
\end{figure}

If data is noisy, it is expected to be harder to construct a meaningful GL-manifold, as some connections within
the graph will be noisy. This is exactly what is illustrated in figure~\ref{fig:clean_manifolds}, where 
different GL-manifold for noisy sinogram (SNR=20dB) and k from 3 to 10. GL-manifold can never express
the true data with a perfect circle. As noise is choosen rather moderate, GL-manifold has still some 
power to express underlying data.


Further, when observing a sinogram, parameter $\theta$ defines how many samples (straight lines) are drawn
and $s$ defines the amount of sampling points. Both together define the expresivness of our sinogram.

\begin{figure}[H]
    \centering
    \subbottom[Clean sinogram GL-manifold, k = 6 and 500 samples \label{fig:clean_manifold_500}]
        {\includegraphics[width=0.25\textwidth]{phaton_clean_manifold_500_k6.png}}
    \subbottom[Clean sinogram GL-manifold, k = 6 and 700 samples \label{fig:clean_manifold_700}]
        {\includegraphics[width=0.25\textwidth]{phaton_clean_manifold_700_k6.png}}
    \caption{Shepp-Logan phantom sinogram GL-manifolds: Importance of number of samples.}
\end{figure}

In figure~\ref{fig:clean_manifold_500} GL-manifold with $\theta \in \mathbb{R}^{500}$ and k=6 is illustrated
for clean sinogram. It looks like 6 are too many neighbours, as the perfect circle cannot be established anymore.
But, if $\theta$ is increased $\theta \in \mathbb{R}^{700}$, more information is available and with the same amount of neighbours,
a perfect circle can be established (figure~\ref{fig:clean_manifold_700}).

\begin{figure}[H]
    \centering
    \subbottom[Clean sinogram GL-manifold, k = 5 and resolution=200 \label{fig:clean_manifold_res200}]
        {\includegraphics[width=0.25\textwidth]{phaton_clean_manifold_res_200_k5.png}}
    \subbottom[Clean sinogram GL-manifold, k = 5 and resolution=400 \label{fig:clean_manifold_res400}]
        {\includegraphics[width=0.25\textwidth]{phaton_clean_manifold_res_400_k5.png}}
    \caption{Shepp-Logan phantom sinogram GL-manifolds: Importance of number of samples.}
\end{figure}

Moreover, the number of sampling points is important as well.
Here, for more sampling points it is expected to be harder to come up with good neighbours (fixing k and number of samples),
as more data need to be explained with the same amount of samples and neighbours and it is more likely, that nodes are connceted wrongly.
This can be seen in figure~\ref{fig:clean_manifold_res200} and figure~\ref{fig:clean_manifold_res400}, where with resolution 
200, the perfect circle can be established and with resolution 400, not anymore (by same parameter k = 5 and $\theta \in \mathbb{R}^{500}$ ).

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    As with the last plots illustrated, the GL-manifold is pretty sensitive.
    Therefore, it is best practise to try different parameters (k) for finding the best 
    looking GL-manifold.
\end{tcolorbox}


\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    In the field of classical tomography and cryo-EM, the underlying low-dimensional manifold is well defined for none-noisy data.
    In the 2D case of classical tomography, the underlying manifold is a circle, whereas in 3D case of cryo-EM the manifold
    is defined as a sphere.
    This fact can be exploited during learning.
\end{tcolorbox}

\subsection{Connection to Machine Learning}

Graph Laplacian is used for dimensionality reduction for high-dimensional data, as well as spectral clustering and semi-supervised learning.
\citet{LaplaceRandomProjections} used Graph Laplacian in a complete other domain, namely in tomography. 
They showed that Graph Laplacian approximates the Laplace-Beltrami operator.
Further, Graph Laplacian is depended on the adjacency matrix $A$, if $A$ is noisy, Graph Laplacian will be noisy as well.


\section{Constructing Graphs for molecular-imaging}

\textbf{TODO: How can we construct a graph (distance between observations, distance between low-dimensional manifold / angles).}