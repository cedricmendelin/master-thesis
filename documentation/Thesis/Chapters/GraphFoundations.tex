\chapter{Manifolds and Graphs}
\label{sec:manifold_and_graphs}
    
\textbf{TODO:}
The following chapter establishes connection between manifolds and graphs 
for CT and cryo-EM. 

Further, 
First, a broad definition of graphs is given and further, the term "Graph Denoising" is
introduced and explained. Finally, link to Graph Laplacian, Manifolds and molecular imaging methods is established.

\section{Graph Foundations}
Real world data is often in a graph structure, like social networks, citation networks,
protein interaction networks or a simple google search. 
If data is not available as a graph structure, a graph can be artificially constructed with k-NN or other methods.

\paragraph{Graph Learning:} 

Graph Learning got a lot of attention in recent years.
The idea is to learn graph information, such as topology or connections between nodes, to solve tasks.
Attention mechanisms are popular at the moment\cite{transformer} and the idea was derived to graphs as well \cite{GAT}.
It resumes to computing node features by using local information, therfore, the neighborhood of nodes.
Popular learning tasks are \textit{node classification} or \textit{link prediction}, where a model is learned from node and edge features 
as well as topology. The model can be used for prediction or classification on nodes or edges.
Another common task is \textit{community detection}, which aims to identify clusters of nodes within the input graph.
Further, graphs are highly favored for \textit{dimensionality-reduction}, where 
graph algorithms provide a helpful tool, as ordinary algorithms like principal component analysis fail to 
establish a meaningful dimensionality reduction.


\paragraph{Constructing Graphs for molecular-imaging:}
For a cryo-EM or CT observation, a graph can be constructed as well.
Every observation $y_i$ can be assigned to a node $v_i$, consequently $v_i \in \mathbb{R}^M$ and $|V|=N$.

To determine distance between two nodes, a distance measure needs to be defined.
For CT, it can be set up by using the $\ell2$-norm $\norm{y_i - y_j}$.
A cryo-EM distance measure is more challenging to setup, as projections are drawn with some random 3D rotation and projection.
It can happen that two observations are equivalent up to a 2D rotation. 
Consider a first observation $y_1$, which has no 3D rotation and 
a second observation $y_2$ with a rotation in x-y plane by 45Â°.
The two projections have a defined in-plane rotation $g$, such that $g \; y_1 = y_2$.
Therefore, a term of in-plane rotation is added to the $\ell2$-norm: $min_{g \in SO(1)}\norm{g \;y_i - y_j}$, 
which is inspired by \cite{multiDiffusionMaps}.



\section{Graph Laplacian \& Manifolds}
In the following section, the connection of Graph Laplacian (GL) and manifolds to CT and cryo-EM is established.

\subsection{Graph Laplacian}
Graph Laplacian (GL) is a matrix that represents a graph and can be used to find many important properties.
It is a very powerful tool and a good introduction and overview can be found in \cite{tutorialSpectralClustering, SpectralGraphTheory}. 

GL is defined as follows:
\begin{equation}
    \label{eq:gl}
    L = D - A,
\end{equation}

Where $A$ is the adjacency matrix and $D$ the degree matrix (diagonal matrix with degree of nodes as entries).


\subsection{Manifold of CT and cryo-EM}
A basic introduction to manifolds and how a low-dimensional embeddings can be computed is found in \ref{sec:manifolds}.

In this section, the connection to CT and cryo-EM is established.
For a given observation, a low-dimensional embedding can be computed by using GL.
In the following, it will be observed how this embedding, the second and third smallest eigenvectors of GL, looks like for CT and cryo-EM observations.
Shepp-Logan phantom is used again as an example for CT.
For Radon Transform, $\theta$ and $s$ are specified with $\theta \in \mathbb{R}^{500}$ as evenly spaced
between $[0, 2 \pi]$ and $dim(s) = 200$. 

In Figure~\ref{fig:clean_manifold}, embedding calculated from clean sinogram and $k=2$ can be seen.
It looks like a perfect circle and angles are in order. 
Further, noise was added to the sinogram to reach SNR 20 dB and 10 dB and the embedding was computed with $k=6$, 
which is illustrated in Figure~\ref{noisy_manifold_k6_snr20} and Figure~\ref{noisy_manifold_k6_snr10}.
For \snry 20 dB, a circle like object could be established and for \snry 10 dB it looks like a half circle. 

\begin{figure}[H]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{phaton_clean_manifold.png}
        \caption{Clean sinogram embedding with $k=2$}
        \label{fig:clean_manifold}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{phaton_noisy_manifold_k6_snr20.png}
      \caption{Noisy sinogram embedding $k=6$ with \snry 20 dB}
      \label{fig:noisy_manifold_k6_snr20}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{phaton_noisy_manifold_k6_snr10.png}
      \caption{Noisy sinogram embedding $k=6$ with \snry 10 dB}
      \label{fig:noisy_manifold_k6_snr10}
    \end{subfigure}
    \caption{Shepp-Logan phantom sinogram GL eigenvectors}
    \label{fig:phantom_manifolds}
  \end{figure}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    In the fields of CT and cryo-EM, underlying low-dimensional embedding from GL is well-defined for noiseless data.
    In 2D the underlying GL-manifold is a circle, whereas in 3D the GL-manifold is defined as a sphere.
    This fact can be exploited during learning.
\end{tcolorbox}

\paragraph{Tomography for unknown angles:}
But what can we use this embedding for?
It is defining a low-dimensional mapping from high-dimensional space, which approximate angles observation direction.
Therefore, for our noisy observations, angles can be approximated and reconstruction can be established even if angles are unknown.
The problem here is the quality of our embedding. As long as the computed embedding is a mapping to the circle (or sphere),
it should be reasonable to do reconstruction with.
In Figure~\ref{fig:phantom_fbp_unknown_angles} reconstruction with unknown angles is applied. The same settings as before, $k=6$
and \snry 20 dB and 10 dB are used. Reconstruction

\begin{figure}[H]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{fbp_phantom_clean_unknown_angles.png}
        \caption{Clean sinogram reconstruction from GL estimated angles.}
        \label{fig:clean_reco_unknown}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{fbp_phantom_snr_20_unknown_angels.png}
      \caption{Noisy sinogram with \snry 20 dB reconstruction from GL estimated angles.}
      \label{fig:noisy_snr20_reco_unknown}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\textwidth]{fbp_phantom_snr_10_unknown_angels.png}
      \caption{Noisy sinogram with \snry 10 dB reconstruction from GL estimated angles.}
      \label{fig:noisy_snr10_reco_unknown}
    \end{subfigure}
    \caption{Shepp-Logan phantom sinogram GL eigenvectors}
    \label{fig:phantom_fbp_unknown_angles}
  \end{figure}


  \paragraph{Denoise observations}
  As the quality of reconstruction is highly dependent on observations and their noise, it
  is expected to increase, when the level of noise is decreased.
  Therefore, standard denoising methods like Block-matching and 3D filtering (BM3D) \cite{bm3d} or 
  non-local means \cite{noneLocalMean} could be used to denoise these observations.
  Both emerged from Signal Processing and are not operating on a graph structure. 
  But, they use a neighborhood for averaging, which shows great potential for graph 
  as a data structure for denoising, as graphs can represent neighborhoods really well.
  BM3D is considered the state-of-the-art denoising algorithm, before algorithms emerged from Deep Learning.
  Therefore, it will be used as a baseline algorithm in the practical part.
  Further, as illustrated with the GL embedding, graphs can restore information with a suitable prior,
  such as angles are uniformly sampled.

\section{Graph Denoising:}
\textit{Graph Denoising} is not a common term in literature.
In the current chapter, a way of constructing a k-NN graph from observations was introduced.
Moreover, the underlying true graph was found by computing the GL embedding, which is a circle or a sphere.
Therefore, our constructed graph from observations can be considered a noisy graph, 
as observations are noisy. 
As a consequence, the constructed noisy graph has some missing and additional edges compared to the true graph.
The goal of Graph Denoising is to estimate original graph $G$ from a given noisy graph $G_0$.
In other words, noisy graph $G_0$ will be denoised.
This is my definition for Graph Denoising, which is rather related to signal or image denoising.
Reconstruction of a true signal given noisy observation signal is done via averaging, that can be performed

For every noisy graph there exists an original graph $G = \langle V,E \rangle$.
The noisy graph $G_0$ can further be defined as $G_0 = \langle V, E_0 \rangle$,  
 where $E_0 = E \setminus  E^{-}_0 \cup  E^{+}_0$ with $E^{-}_0 \subseteq E$ and $E^{+}_0 \cap E = \emptyset$.

$G_0$ consists of same nodes $V$ as original graph $G$. 
From $E$ some edges are removed (denoted by $E^{-}_0$) and some are added
(denoted by $E^{+}_0$), which results in edges $E_0$.

Graph Denoising can therefore be written as 
\begin{equation}
    GD: A_0 \mapsto \tilde{A} \approx A
\end{equation}

Where $A_0$, $\tilde{A}$, $A$ denotes adjacency matrix from noisy input graph, denoised graph and original graph respectively.


\paragraph{Connection to link prediction:}
Link prediction is a task in Graph Learning. 
The goal is to predict existence of a link between two nodes.
The task can be formulated as a missing value estimation task. A model $M_p$ is learned
from a given set of observed edges. The model finally maps links to probabilities
$M_p : E^{\prime} \rightarrow [0,1]$ where $E^{\prime}$ is the set of potential links.

Further, $U$ determines the set of all possible vertices of $G$, therefore $E \subseteq U$.
Clearly, Graph Denoising can be seen as a link prediction problem.
The difference is, that in link prediction a model from a set of observed links is learned
$E^{\prime} \subseteq E$ and in Graph Denoising model is learned from 
$E^{\prime} \subseteq U$. 

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    Link prediction problems are a subset of graph denoising problems.
\end{tcolorbox}

\section{Graph Deep Learning}
\label{sec:graph_depp_learning}
Graph Denoising can be seen as a way of link predication. 
The state-of-the-art methods for solving link prediction are \textit{Graph Deep Learning} approaches.
With Graph Neural Networks (GNN)~\cite{GNN} the framework
for neural networks with graphs has been established. 

Using Graph Convolutional Networks (GCN)~\cite{GCN} for graph feature extraction is a popular way. 
With GCN a new feature representation is iteratively learned for node features (edge features are not considered).
It can be seen as an averaging of nodes over their neighborhood where all neighbors get the same weight combined with some non-linear activation (e.g. ReLu). 
To consider the node itself in averaging, \citet{GCN} applies the so-called "Renormalization trick", where self-loops are added to the 
adjacency matrix and after every layer, a normalization step is applied. 
The topology of the graph will not be adjusted during the learning process.

Simple Graph Convolutional Network~\cite{simpleGCN} proposed a simplified version of GCN.
They could verify their hypothesis that GCN is dominated by local averaging step and non-linear 
activation function between layers do not contribute too much to the success of GCN. 
Therefore, it can be seen as a way of power iteration\footnote{For more details see \ref{sec:powerIterations}}
over the adjacency matrix with normalization in every layer.
\citet{dynamicGCN} proposed an extension to GCN by not operating on the same graph in every layer but adopting
underlying graph topology layer by layer.
Graph Attention Networks (GAT) \cite{GAT} extended the concept of GCN with attention where not
all neighboring nodes get the same weight (attention).
